{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader,Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.node_parser import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiohappyeyeballs 2.4.0: Mon Sep  2 00:19:14 2024\n",
      "aiohttp 3.10.5: Mon Sep  2 00:20:06 2024\n",
      "aiosignal 1.3.1: Mon Sep  2 00:19:59 2024\n",
      "annotated-types 0.7.0: Mon Sep  2 00:19:14 2024\n",
      "anyio 3.7.0: Wed Jun  7 22:09:57 2023\n",
      "argon2-cffi 21.3.0: Wed Jun  7 22:10:00 2023\n",
      "argon2-cffi-bindings 21.2.0: Wed Jun  7 22:09:58 2023\n",
      "arrow 1.2.3: Wed Jun  7 22:09:58 2023\n",
      "asgiref 3.8.1: Mon Sep  2 00:19:59 2024\n",
      "asttokens 2.2.1: Wed Jun  7 22:09:57 2023\n",
      "async-timeout 4.0.3: Mon Sep  2 00:19:14 2024\n",
      "attrs 23.1.0: Wed Jun  7 22:09:53 2023\n",
      "backcall 0.2.0: Wed Jun  7 22:09:42 2023\n",
      "backoff 2.2.1: Mon Sep  2 00:19:14 2024\n",
      "bcrypt 4.2.0: Mon Sep  2 00:19:14 2024\n",
      "beautifulsoup4 4.12.3: Mon Sep  2 00:19:14 2024\n",
      "bleach 6.0.0: Wed Jun  7 22:09:56 2023\n",
      "boto3 1.35.10: Mon Sep  2 00:20:23 2024\n",
      "botocore 1.35.10: Mon Sep  2 00:19:58 2024\n",
      "build 1.2.1: Mon Sep  2 00:19:57 2024\n",
      "cachetools 5.5.0: Mon Sep  2 00:19:13 2024\n",
      "certifi 2024.8.30: Mon Sep  2 00:19:13 2024\n",
      "cffi 1.15.1: Wed Jun  7 22:09:56 2023\n",
      "charset-normalizer 3.3.2: Mon Sep  2 00:19:13 2024\n",
      "chromadb 0.5.3: Mon Sep  2 00:21:00 2024\n",
      "chroma-hnswlib 0.7.3: Mon Sep  2 00:19:13 2024\n",
      "click 8.1.7: Mon Sep  2 00:19:13 2024\n",
      "colorama 0.4.6: Wed Jun  7 22:09:53 2023\n",
      "coloredlogs 15.0.1: Mon Sep  2 00:19:56 2024\n",
      "comm 0.1.3: Wed Jun  7 22:09:56 2023\n",
      "contourpy 1.2.1: Sat Apr 20 00:18:05 2024\n",
      "cycler 0.12.1: Sat Apr 20 00:18:05 2024\n",
      "dataclasses-json 0.6.7: Mon Sep  2 00:20:06 2024\n",
      "debugpy 1.6.7: Wed Jun  7 22:09:50 2023\n",
      "decorator 5.1.1: Wed Jun  7 22:09:50 2023\n",
      "defusedxml 0.7.1: Wed Jun  7 22:09:50 2023\n",
      "Deprecated 1.2.14: Mon Sep  2 00:19:56 2024\n",
      "dirtyjson 1.0.8: Mon Sep  2 00:18:44 2024\n",
      "distro 1.9.0: Mon Sep  2 00:19:13 2024\n",
      "et-xmlfile 1.1.0: Fri Apr 19 20:18:59 2024\n",
      "exceptiongroup 1.1.1: Wed Jun  7 22:09:50 2023\n",
      "executing 1.2.0: Wed Jun  7 22:09:42 2023\n",
      "fastapi 0.112.2: Mon Sep  2 00:20:22 2024\n",
      "fastjsonschema 2.17.1: Wed Jun  7 22:09:42 2023\n",
      "filelock 3.15.4: Mon Sep  2 00:19:13 2024\n",
      "flatbuffers 24.3.25: Mon Sep  2 00:18:44 2024\n",
      "fonttools 4.51.0: Sat Apr 20 00:18:03 2024\n",
      "fqdn 1.5.1: Wed Jun  7 22:09:50 2023\n",
      "frozenlist 1.4.1: Mon Sep  2 00:19:13 2024\n",
      "fsspec 2024.6.1: Mon Sep  2 00:19:12 2024\n",
      "googleapis-common-protos 1.65.0: Mon Sep  2 00:19:55 2024\n",
      "google-auth 2.34.0: Mon Sep  2 00:20:05 2024\n",
      "greenlet 3.0.3: Mon Sep  2 00:19:12 2024\n",
      "groq 0.10.0: Mon Sep  2 00:20:22 2024\n",
      "grpcio 1.66.1: Mon Sep  2 00:19:11 2024\n",
      "h11 0.14.0: Mon Sep  2 00:19:11 2024\n",
      "httpcore 1.0.5: Mon Sep  2 00:19:55 2024\n",
      "httptools 0.6.1: Mon Sep  2 00:19:11 2024\n",
      "httpx 0.27.2: Mon Sep  2 00:20:05 2024\n",
      "huggingface-hub 0.24.6: Mon Sep  2 00:20:04 2024\n",
      "humanfriendly 10.0: Mon Sep  2 00:19:11 2024\n",
      "idna 3.4: Wed Jun  7 22:09:50 2023\n",
      "importlib_metadata 8.4.0: Mon Sep  2 00:19:55 2024\n",
      "importlib_resources 6.4.4: Mon Sep  2 00:19:11 2024\n",
      "iniconfig 2.0.0: Mon Sep  2 00:19:11 2024\n",
      "InstructorEmbedding 1.0.1: Mon Sep  9 00:09:15 2024\n",
      "intel-openmp 2021.4.0: Mon Sep  2 00:34:44 2024\n",
      "ipykernel 6.23.1: Wed Jun  7 22:10:01 2023\n",
      "ipython 8.14.0: Wed Jun  7 22:09:58 2023\n",
      "ipython-genutils 0.2.0: Wed Jun  7 22:09:42 2023\n",
      "ipywidgets 8.0.6: Wed Jun  7 22:10:02 2023\n",
      "isoduration 20.11.0: Wed Jun  7 22:09:58 2023\n",
      "jedi 0.18.2: Wed Jun  7 22:09:55 2023\n",
      "Jinja2 3.1.2: Wed Jun  7 22:09:54 2023\n",
      "jiter 0.5.0: Mon Sep  2 00:19:11 2024\n",
      "jmespath 1.0.1: Mon Sep  2 00:19:10 2024\n",
      "joblib 1.4.0: Sat Apr 20 00:40:43 2024\n",
      "jsonpatch 1.33: Mon Sep  2 00:19:10 2024\n",
      "jsonpointer 2.3: Wed Jun  7 22:09:50 2023\n",
      "jsonschema 4.17.3: Wed Jun  7 22:09:54 2023\n",
      "jupyter 1.0.0: Wed Jun  7 22:10:04 2023\n",
      "jupyterlab-pygments 0.2.2: Wed Jun  7 22:09:50 2023\n",
      "jupyterlab-widgets 3.0.7: Wed Jun  7 22:09:50 2023\n",
      "jupyter_client 8.2.0: Wed Jun  7 22:09:57 2023\n",
      "jupyter-console 6.6.3: Wed Jun  7 22:10:01 2023\n",
      "jupyter_core 5.3.0: Wed Jun  7 22:09:54 2023\n",
      "jupyter-events 0.6.3: Wed Jun  7 22:10:01 2023\n",
      "jupyter_server 2.6.0: Wed Jun  7 22:10:02 2023\n",
      "jupyter_server_terminals 0.4.4: Wed Jun  7 22:09:57 2023\n",
      "kiwisolver 1.4.5: Sat Apr 20 00:18:02 2024\n",
      "kubernetes 30.1.0: Mon Sep  2 00:20:16 2024\n",
      "langchain 0.2.15: Mon Sep  2 00:20:51 2024\n",
      "langchain-chroma 0.1.3: Mon Sep  2 00:21:10 2024\n",
      "langchain-community 0.2.15: Mon Sep  2 00:21:02 2024\n",
      "langchain-core 0.2.37: Mon Sep  2 00:20:48 2024\n",
      "langchain-groq 0.1.9: Mon Sep  2 00:20:51 2024\n",
      "langchain-huggingface 0.0.3: Mon Sep  2 00:20:51 2024\n",
      "langchain-text-splitters 0.2.2: Mon Sep  2 00:20:51 2024\n",
      "langsmith 0.1.108: Mon Sep  2 00:20:15 2024\n",
      "llama-cloud 0.0.15: Mon Sep  2 00:20:14 2024\n",
      "llama-index 0.11.7: Mon Sep  9 02:25:37 2024\n",
      "llama-index-agent-openai 0.3.1: Mon Sep  9 02:25:37 2024\n",
      "llama-index-cli 0.3.0: Mon Sep  2 00:20:50 2024\n",
      "llama-index-core 0.11.7: Mon Sep  9 02:25:34 2024\n",
      "llama-index-embeddings-huggingface 0.3.1: Mon Sep  9 00:09:12 2024\n",
      "llama-index-embeddings-instructor 0.2.1: Mon Sep  9 00:09:15 2024\n",
      "llama-index-embeddings-openai 0.2.4: Mon Sep  9 02:25:37 2024\n",
      "llama-index-indices-managed-llama-cloud 0.3.0: Mon Sep  2 00:20:48 2024\n",
      "llama-index-legacy 0.9.48.post3: Mon Sep  2 00:20:43 2024\n",
      "llama-index-llms-groq 0.2.0: Mon Sep  2 02:19:14 2024\n",
      "llama-index-llms-openai 0.2.3: Mon Sep  9 02:25:37 2024\n",
      "llama-index-llms-openai-like 0.2.0: Mon Sep  2 02:19:14 2024\n",
      "llama-index-multi-modal-llms-openai 0.2.0: Mon Sep  2 00:20:50 2024\n",
      "llama-index-program-openai 0.2.0: Mon Sep  2 00:20:51 2024\n",
      "llama-index-question-gen-openai 0.2.0: Mon Sep  2 00:21:01 2024\n",
      "llama-index-readers-file 0.2.0: Mon Sep  2 00:20:42 2024\n",
      "llama-index-readers-llama-parse 0.3.0: Mon Sep  9 02:25:37 2024\n",
      "llama-index-vector-stores-chroma 0.2.0: Sun Sep  8 23:17:16 2024\n",
      "llama-parse 0.5.1: Mon Sep  2 00:20:42 2024\n",
      "lxml 5.3.0: Fri Sep 13 23:39:26 2024\n",
      "markdown-it-py 3.0.0: Mon Sep  2 00:19:54 2024\n",
      "MarkupSafe 2.1.3: Wed Jun  7 22:09:50 2023\n",
      "marshmallow 3.22.0: Mon Sep  2 00:19:54 2024\n",
      "matplotlib 3.8.4: Sat Apr 20 00:18:05 2024\n",
      "matplotlib-inline 0.1.6: Wed Jun  7 22:09:54 2023\n",
      "mdurl 0.1.2: Mon Sep  2 00:19:10 2024\n",
      "minijinja 2.2.0: Mon Sep  9 00:09:12 2024\n",
      "mistune 2.0.5: Wed Jun  7 22:09:42 2023\n",
      "mkl 2021.4.0: Mon Sep  2 00:34:44 2024\n",
      "mlxtend 0.23.1: Sat Apr 20 10:10:28 2024\n",
      "mmh3 4.1.0: Mon Sep  2 00:18:44 2024\n",
      "monotonic 1.6: Mon Sep  2 00:18:44 2024\n",
      "mpmath 1.3.0: Mon Sep  2 00:18:43 2024\n",
      "multidict 6.0.5: Mon Sep  2 00:19:10 2024\n",
      "mypy-extensions 1.0.0: Mon Sep  2 00:19:10 2024\n",
      "nbclassic 1.0.0: Wed Jun  7 22:10:03 2023\n",
      "nbclient 0.8.0: Wed Jun  7 22:09:58 2023\n",
      "nbconvert 7.4.0: Wed Jun  7 22:10:00 2023\n",
      "nbformat 5.9.0: Wed Jun  7 22:09:57 2023\n",
      "nest-asyncio 1.6.0: Mon Sep  2 00:19:10 2024\n",
      "networkx 3.3: Mon Sep  2 00:19:04 2024\n",
      "nltk 3.9.1: Mon Sep  2 00:19:50 2024\n",
      "notebook 6.5.4: Wed Jun  7 22:10:04 2023\n",
      "notebook_shim 0.2.3: Wed Jun  7 22:10:02 2023\n",
      "numpy 1.26.4: Fri Apr 19 20:15:43 2024\n",
      "oauthlib 3.2.2: Mon Sep  2 00:19:03 2024\n",
      "onnxruntime 1.19.0: Mon Sep  2 00:20:02 2024\n",
      "openai 1.43.0: Mon Sep  2 00:20:08 2024\n",
      "openpyxl 3.1.2: Fri Apr 19 20:18:59 2024\n",
      "opentelemetry-api 1.27.0: Mon Sep  2 00:20:01 2024\n",
      "opentelemetry-exporter-otlp-proto-common 1.27.0: Mon Sep  2 00:20:01 2024\n",
      "opentelemetry-exporter-otlp-proto-grpc 1.27.0: Mon Sep  2 00:20:50 2024\n",
      "opentelemetry-instrumentation 0.48b0: Mon Sep  2 00:20:07 2024\n",
      "opentelemetry-instrumentation-asgi 0.48b0: Mon Sep  2 00:20:42 2024\n",
      "opentelemetry-instrumentation-fastapi 0.48b0: Mon Sep  2 00:20:50 2024\n",
      "opentelemetry-proto 1.27.0: Mon Sep  2 00:19:50 2024\n",
      "opentelemetry-sdk 1.27.0: Mon Sep  2 00:20:42 2024\n",
      "opentelemetry-semantic-conventions 0.48b0: Mon Sep  2 00:20:07 2024\n",
      "opentelemetry-util-http 0.48b0: Mon Sep  2 00:19:03 2024\n",
      "orjson 3.10.7: Mon Sep  2 00:19:03 2024\n",
      "overrides 7.3.1: Wed Jun  7 22:09:49 2023\n",
      "packaging 24.1: Mon Sep  2 00:19:03 2024\n",
      "pandas 2.2.2: Fri Apr 19 20:15:47 2024\n",
      "pandocfilters 1.5.0: Wed Jun  7 22:09:49 2023\n",
      "parso 0.8.3: Wed Jun  7 22:09:49 2023\n",
      "patsy 0.5.6: Sat Apr 20 00:56:53 2024\n",
      "pickleshare 0.7.5: Wed Jun  7 22:09:42 2023\n",
      "pillow 10.3.0: Sat Apr 20 00:18:02 2024\n",
      "pip 24.0: Fri Apr 19 20:17:31 2024\n",
      "pip-date 1.0.5: Fri Sep 13 23:39:26 2024\n",
      "platformdirs 3.5.1: Wed Jun  7 22:09:49 2023\n",
      "pluggy 1.5.0: Mon Sep  2 00:19:03 2024\n",
      "posthog 3.6.0: Mon Sep  2 00:20:01 2024\n",
      "prometheus-client 0.17.0: Wed Jun  7 22:09:49 2023\n",
      "prompt-toolkit 3.0.38: Wed Jun  7 22:09:48 2023\n",
      "protobuf 4.25.4: Mon Sep  2 00:19:02 2024\n",
      "psutil 5.9.5: Wed Jun  7 22:09:47 2023\n",
      "pure-eval 0.2.2: Wed Jun  7 22:09:42 2023\n",
      "pyasn1 0.6.0: Mon Sep  2 00:19:02 2024\n",
      "pyasn1_modules 0.4.0: Mon Sep  2 00:19:48 2024\n",
      "pycparser 2.21: Wed Jun  7 22:09:47 2023\n",
      "pydantic 2.8.2: Mon Sep  2 00:20:00 2024\n",
      "pydantic_core 2.20.1: Mon Sep  2 00:19:48 2024\n",
      "Pygments 2.15.1: Wed Jun  7 22:09:45 2023\n",
      "pyparsing 3.1.2: Sat Apr 20 00:18:01 2024\n",
      "pypdf 4.3.1: Mon Sep  2 00:19:47 2024\n",
      "PyPika 0.48.9: Mon Sep  2 00:18:43 2024\n",
      "pyproject_hooks 1.1.0: Mon Sep  2 00:19:02 2024\n",
      "pyreadline3 3.4.1: Mon Sep  2 00:18:42 2024\n",
      "pyrsistent 0.19.3: Wed Jun  7 22:09:44 2023\n",
      "pytest 8.3.2: Mon Sep  2 00:19:47 2024\n",
      "python-dateutil 2.8.2: Wed Jun  7 22:09:54 2023\n",
      "python-dotenv 1.0.1: Mon Sep  2 00:19:02 2024\n",
      "python-json-logger 2.0.7: Wed Jun  7 22:09:44 2023\n",
      "pytz 2024.1: Fri Apr 19 20:15:42 2024\n",
      "pywin32 306: Wed Jun  7 22:09:39 2023\n",
      "pywinpty 2.0.10: Wed Jun  7 22:09:44 2023\n",
      "PyYAML 6.0.2: Mon Sep  2 00:19:01 2024\n",
      "pyzmq 25.1.0: Wed Jun  7 22:09:43 2023\n",
      "qtconsole 5.4.3: Wed Jun  7 22:10:01 2023\n",
      "QtPy 2.3.1: Wed Jun  7 22:09:53 2023\n",
      "regex 2024.7.24: Mon Sep  2 00:19:01 2024\n",
      "requests 2.32.3: Mon Sep  2 00:19:46 2024\n",
      "requests-oauthlib 2.0.0: Mon Sep  2 00:20:00 2024\n",
      "rfc3339-validator 0.1.4: Wed Jun  7 22:09:53 2023\n",
      "rfc3986-validator 0.1.1: Wed Jun  7 22:09:43 2023\n",
      "rich 13.8.0: Mon Sep  2 00:19:59 2024\n",
      "rsa 4.9: Mon Sep  2 00:19:46 2024\n",
      "s3transfer 0.10.2: Mon Sep  2 00:19:59 2024\n",
      "safetensors 0.4.4: Mon Sep  2 00:19:01 2024\n",
      "scikit-learn 1.4.2: Sat Apr 20 00:40:44 2024\n",
      "scipy 1.13.0: Sat Apr 20 00:40:36 2024\n",
      "seaborn 0.13.2: Sat Apr 20 09:54:42 2024\n",
      "Send2Trash 1.8.2: Wed Jun  7 22:09:43 2023\n",
      "sentence-transformers 2.7.0: Mon Sep  9 00:09:15 2024\n",
      "setuptools 65.5.0: Wed Jun  7 22:08:57 2023\n",
      "shellingham 1.5.4: Mon Sep  2 00:19:01 2024\n",
      "six 1.16.0: Wed Jun  7 22:09:43 2023\n",
      "sniffio 1.3.0: Wed Jun  7 22:09:43 2023\n",
      "soupsieve 2.4.1: Wed Jun  7 22:09:43 2023\n",
      "SQLAlchemy 2.0.32: Mon Sep  2 00:19:43 2024\n",
      "stack-data 0.6.2: Wed Jun  7 22:09:57 2023\n",
      "starlette 0.38.4: Mon Sep  2 00:19:42 2024\n",
      "statsmodels 0.14.2: Sat Apr 20 00:56:54 2024\n",
      "striprtf 0.0.26: Mon Sep  2 00:18:42 2024\n",
      "sympy 1.13.2: Mon Sep  2 00:18:46 2024\n",
      "tbb 2021.13.1: Mon Sep  2 00:34:44 2024\n",
      "tenacity 8.5.0: Mon Sep  2 00:18:45 2024\n",
      "terminado 0.17.1: Wed Jun  7 22:09:53 2023\n",
      "threadpoolctl 3.4.0: Sat Apr 20 00:40:35 2024\n",
      "tiktoken 0.7.0: Mon Sep  2 00:19:59 2024\n",
      "tinycss2 1.2.1: Wed Jun  7 22:09:43 2023\n",
      "tokenizers 0.19.1: Mon Sep  2 00:20:07 2024\n",
      "tomli 2.0.1: Mon Sep  2 00:18:45 2024\n",
      "torch 2.3.0: Mon Sep  2 00:34:47 2024\n",
      "tornado 6.3.2: Wed Jun  7 22:09:43 2023\n",
      "tqdm 4.66.5: Mon Sep  2 00:18:45 2024\n",
      "traitlets 5.9.0: Wed Jun  7 22:09:42 2023\n",
      "transformers 4.44.2: Mon Sep  2 00:20:24 2024\n",
      "typer 0.12.5: Mon Sep  2 00:20:06 2024\n",
      "typing_extensions 4.12.2: Mon Sep  2 00:18:45 2024\n",
      "typing-inspect 0.9.0: Mon Sep  2 00:19:15 2024\n",
      "tzdata 2024.1: Fri Apr 19 20:15:42 2024\n",
      "uri-template 1.2.0: Wed Jun  7 22:09:42 2023\n",
      "urllib3 2.2.2: Mon Sep  2 00:18:45 2024\n",
      "uvicorn 0.30.6: Mon Sep  2 00:19:14 2024\n",
      "watchfiles 0.24.0: Mon Sep  2 00:19:14 2024\n",
      "wcwidth 0.2.6: Wed Jun  7 22:09:39 2023\n",
      "webcolors 1.13: Wed Jun  7 22:09:42 2023\n",
      "webencodings 0.5.1: Wed Jun  7 22:09:39 2023\n",
      "websockets 13.0.1: Mon Sep  2 00:18:44 2024\n",
      "websocket-client 1.5.2: Wed Jun  7 22:09:42 2023\n",
      "widgetsnbextension 4.0.7: Wed Jun  7 22:09:42 2023\n",
      "wrapt 1.16.0: Mon Sep  2 00:18:44 2024\n",
      "yarl 1.9.6: Mon Sep  2 00:19:14 2024\n",
      "zipp 3.20.1: Mon Sep  2 00:18:44 2024\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import distributions  \n",
    "import os, time\n",
    "\n",
    "for dist in distributions():\n",
    "    print(\"%s %s: %s\" % (dist.metadata[\"Name\"], dist.version, time.ctime(os.path.getctime(dist._path))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiohappyeyeballs 2.4.0: Fri Sep 13 23:33:45 2024\n",
      "aiohttp 3.10.5: Fri Sep 13 23:35:13 2024\n",
      "aiosignal 1.3.1: Fri Sep 13 23:34:05 2024\n",
      "annotated-types 0.7.0: Fri Sep 13 23:33:45 2024\n",
      "anyio 4.4.0: Fri Sep 13 23:34:05 2024\n",
      "asgiref 3.8.1: Fri Sep 13 23:34:05 2024\n",
      "asttokens 2.4.1: Fri Sep 13 23:37:44 2024\n",
      "async-timeout 4.0.3: Fri Sep 13 23:33:45 2024\n",
      "attrs 24.2.0: Fri Sep 13 23:33:45 2024\n",
      "backoff 2.2.1: Fri Sep 13 23:33:45 2024\n",
      "bcrypt 4.2.0: Fri Sep 13 23:33:45 2024\n",
      "beautifulsoup4 4.12.3: Fri Sep 13 23:34:04 2024\n",
      "boto3 1.35.18: Fri Sep 13 23:35:45 2024\n",
      "botocore 1.35.18: Fri Sep 13 23:34:57 2024\n",
      "build 1.2.2: Fri Sep 13 23:34:04 2024\n",
      "cachetools 5.5.0: Fri Sep 13 23:33:45 2024\n",
      "certifi 2024.8.30: Fri Sep 13 23:33:45 2024\n",
      "charset-normalizer 3.3.2: Fri Sep 13 23:33:45 2024\n",
      "chromadb 0.5.5: Fri Sep 13 23:35:55 2024\n",
      "chroma-hnswlib 0.7.6: Fri Sep 13 23:34:04 2024\n",
      "click 8.1.7: Fri Sep 13 23:34:04 2024\n",
      "colorama 0.4.6: Fri Sep 13 23:33:45 2024\n",
      "coloredlogs 15.0.1: Fri Sep 13 23:34:04 2024\n",
      "comm 0.2.2: Fri Sep 13 23:37:46 2024\n",
      "dataclasses-json 0.6.7: Fri Sep 13 23:34:56 2024\n",
      "debugpy 1.8.5: Fri Sep 13 23:37:41 2024\n",
      "decorator 5.1.1: Fri Sep 13 23:37:41 2024\n",
      "Deprecated 1.2.14: Fri Sep 13 23:34:04 2024\n",
      "dirtyjson 1.0.8: Fri Sep 13 23:33:06 2024\n",
      "distro 1.9.0: Fri Sep 13 23:33:45 2024\n",
      "exceptiongroup 1.2.2: Fri Sep 13 23:33:45 2024\n",
      "executing 2.1.0: Fri Sep 13 23:37:41 2024\n",
      "fastapi 0.114.1: Fri Sep 13 23:35:12 2024\n",
      "filelock 3.16.0: Fri Sep 13 23:33:45 2024\n",
      "flatbuffers 24.3.25: Fri Sep 13 23:33:06 2024\n",
      "frozenlist 1.4.1: Fri Sep 13 23:33:44 2024\n",
      "fsspec 2024.9.0: Fri Sep 13 23:33:44 2024\n",
      "googleapis-common-protos 1.65.0: Fri Sep 13 23:34:03 2024\n",
      "google-auth 2.34.0: Fri Sep 13 23:34:55 2024\n",
      "greenlet 3.1.0: Fri Sep 13 23:33:44 2024\n",
      "groq 0.11.0: Fri Sep 13 23:35:12 2024\n",
      "grpcio 1.66.1: Fri Sep 13 23:33:43 2024\n",
      "h11 0.14.0: Fri Sep 13 23:33:43 2024\n",
      "httpcore 1.0.5: Fri Sep 13 23:34:03 2024\n",
      "httptools 0.6.1: Fri Sep 13 23:33:43 2024\n",
      "httpx 0.27.2: Fri Sep 13 23:34:55 2024\n",
      "huggingface-hub 0.24.7: Fri Sep 13 23:34:54 2024\n",
      "humanfriendly 10.0: Fri Sep 13 23:33:42 2024\n",
      "idna 3.8: Fri Sep 13 23:33:42 2024\n",
      "importlib_metadata 8.4.0: Fri Sep 13 23:34:03 2024\n",
      "importlib_resources 6.4.5: Fri Sep 13 23:33:42 2024\n",
      "iniconfig 2.0.0: Fri Sep 13 23:33:42 2024\n",
      "InstructorEmbedding 1.0.1: Fri Sep 13 23:33:06 2024\n",
      "intel-openmp 2021.4.0: Fri Sep 13 23:33:06 2024\n",
      "ipykernel 6.29.5: Fri Sep 13 23:37:49 2024\n",
      "ipython 8.27.0: Fri Sep 13 23:37:47 2024\n",
      "jedi 0.19.1: Fri Sep 13 23:37:45 2024\n",
      "Jinja2 3.1.4: Fri Sep 13 23:34:02 2024\n",
      "jiter 0.5.0: Fri Sep 13 23:33:42 2024\n",
      "jmespath 1.0.1: Fri Sep 13 23:33:42 2024\n",
      "joblib 1.4.2: Fri Sep 13 23:33:41 2024\n",
      "jsonpatch 1.33: Fri Sep 13 23:34:02 2024\n",
      "jsonpointer 3.0.0: Fri Sep 13 23:33:41 2024\n",
      "jupyter_client 8.6.2: Fri Sep 13 23:37:46 2024\n",
      "jupyter_core 5.7.2: Fri Sep 13 23:37:44 2024\n",
      "kubernetes 30.1.0: Fri Sep 13 23:35:05 2024\n",
      "langchain 0.2.16: Fri Sep 13 23:35:47 2024\n",
      "langchain-chroma 0.1.2: Fri Sep 13 23:36:07 2024\n",
      "langchain-community 0.2.17: Fri Sep 13 23:35:57 2024\n",
      "langchain-core 0.2.40: Fri Sep 13 23:35:43 2024\n",
      "langchain-groq 0.1.10: Fri Sep 13 23:35:46 2024\n",
      "langchain-huggingface 0.0.3: Fri Sep 13 23:35:46 2024\n",
      "langchain-text-splitters 0.2.4: Fri Sep 13 23:35:46 2024\n",
      "langsmith 0.1.120: Fri Sep 13 23:35:04 2024\n",
      "llama-cloud 0.0.17: Fri Sep 13 23:35:02 2024\n",
      "llama-index 0.11.9: Fri Sep 13 23:36:08 2024\n",
      "llama-index-agent-openai 0.3.1: Fri Sep 13 23:36:08 2024\n",
      "llama-index-cli 0.3.1: Fri Sep 13 23:36:08 2024\n",
      "llama-index-core 0.11.9: Fri Sep 13 23:35:39 2024\n",
      "llama-index-embeddings-instructor 0.2.1: Fri Sep 13 23:35:46 2024\n",
      "llama-index-embeddings-langchain 0.2.1: Fri Sep 13 23:35:46 2024\n",
      "llama-index-embeddings-openai 0.2.4: Fri Sep 13 23:35:46 2024\n",
      "llama-index-indices-managed-llama-cloud 0.3.0: Fri Sep 13 23:35:46 2024\n",
      "llama-index-legacy 0.9.48.post3: Fri Sep 13 23:35:33 2024\n",
      "llama-index-llms-groq 0.2.0: Fri Sep 13 23:36:08 2024\n",
      "llama-index-llms-openai 0.2.6: Fri Sep 13 23:36:08 2024\n",
      "llama-index-llms-openai-like 0.2.0: Fri Sep 13 23:36:08 2024\n",
      "llama-index-multi-modal-llms-openai 0.2.0: Fri Sep 13 23:36:08 2024\n",
      "llama-index-program-openai 0.2.0: Fri Sep 13 23:36:08 2024\n",
      "llama-index-question-gen-openai 0.2.0: Fri Sep 13 23:36:08 2024\n",
      "llama-index-readers-file 0.2.1: Fri Sep 13 23:35:46 2024\n",
      "llama-index-readers-llama-parse 0.3.0: Fri Sep 13 23:35:46 2024\n",
      "llama-index-vector-stores-chroma 0.2.0: Fri Sep 13 23:35:57 2024\n",
      "llama-parse 0.5.5: Fri Sep 13 23:35:46 2024\n",
      "markdown-it-py 3.0.0: Fri Sep 13 23:34:02 2024\n",
      "MarkupSafe 2.1.5: Fri Sep 13 23:33:41 2024\n",
      "marshmallow 3.22.0: Fri Sep 13 23:34:02 2024\n",
      "matplotlib-inline 0.1.7: Fri Sep 13 23:37:44 2024\n",
      "mdurl 0.1.2: Fri Sep 13 23:33:41 2024\n",
      "mkl 2021.4.0: Fri Sep 13 23:33:38 2024\n",
      "mmh3 4.1.0: Fri Sep 13 23:33:06 2024\n",
      "monotonic 1.6: Fri Sep 13 23:33:06 2024\n",
      "mpmath 1.3.0: Fri Sep 13 23:33:05 2024\n",
      "multidict 6.1.0: Fri Sep 13 23:34:02 2024\n",
      "mypy-extensions 1.0.0: Fri Sep 13 23:33:38 2024\n",
      "nest-asyncio 1.6.0: Fri Sep 13 23:33:38 2024\n",
      "networkx 3.3: Fri Sep 13 23:33:34 2024\n",
      "nltk 3.9.1: Fri Sep 13 23:34:51 2024\n",
      "numpy 1.26.4: Fri Sep 13 23:33:29 2024\n",
      "oauthlib 3.2.2: Fri Sep 13 23:33:28 2024\n",
      "onnxruntime 1.19.2: Fri Sep 13 23:34:48 2024\n",
      "openai 1.45.0: Fri Sep 13 23:34:59 2024\n",
      "opentelemetry-api 1.27.0: Fri Sep 13 23:34:48 2024\n",
      "opentelemetry-exporter-otlp-proto-common 1.27.0: Fri Sep 13 23:34:48 2024\n",
      "opentelemetry-exporter-otlp-proto-grpc 1.27.0: Fri Sep 13 23:35:46 2024\n",
      "opentelemetry-instrumentation 0.48b0: Fri Sep 13 23:34:59 2024\n",
      "opentelemetry-instrumentation-asgi 0.48b0: Fri Sep 13 23:35:33 2024\n",
      "opentelemetry-instrumentation-fastapi 0.48b0: Fri Sep 13 23:35:46 2024\n",
      "opentelemetry-proto 1.27.0: Fri Sep 13 23:34:01 2024\n",
      "opentelemetry-sdk 1.27.0: Fri Sep 13 23:35:33 2024\n",
      "opentelemetry-semantic-conventions 0.48b0: Fri Sep 13 23:34:58 2024\n",
      "opentelemetry-util-http 0.48b0: Fri Sep 13 23:33:28 2024\n",
      "orjson 3.10.7: Fri Sep 13 23:33:28 2024\n",
      "overrides 7.7.0: Fri Sep 13 23:33:28 2024\n",
      "packaging 24.1: Fri Sep 13 23:33:28 2024\n",
      "pandas 2.2.2: Fri Sep 13 23:34:36 2024\n",
      "parso 0.8.4: Fri Sep 13 23:37:41 2024\n",
      "pillow 10.4.0: Fri Sep 13 23:33:27 2024\n",
      "pip 22.3.1: Fri Sep 13 23:32:22 2024\n",
      "platformdirs 4.3.2: Fri Sep 13 23:37:41 2024\n",
      "pluggy 1.5.0: Fri Sep 13 23:33:27 2024\n",
      "posthog 3.6.5: Fri Sep 13 23:34:35 2024\n",
      "prompt_toolkit 3.0.47: Fri Sep 13 23:37:40 2024\n",
      "protobuf 4.25.4: Fri Sep 13 23:33:26 2024\n",
      "psutil 6.0.0: Fri Sep 13 23:37:39 2024\n",
      "pure_eval 0.2.3: Fri Sep 13 23:37:37 2024\n",
      "pyasn1 0.6.1: Fri Sep 13 23:33:26 2024\n",
      "pyasn1_modules 0.4.1: Fri Sep 13 23:34:00 2024\n",
      "pydantic 2.9.1: Fri Sep 13 23:34:34 2024\n",
      "pydantic_core 2.23.3: Fri Sep 13 23:34:00 2024\n",
      "Pygments 2.18.0: Fri Sep 13 23:33:23 2024\n",
      "pypdf 4.3.1: Fri Sep 13 23:34:00 2024\n",
      "PyPika 0.48.9: Fri Sep 13 23:33:05 2024\n",
      "pyproject_hooks 1.1.0: Fri Sep 13 23:33:23 2024\n",
      "pyreadline3 3.4.3: Fri Sep 13 23:33:05 2024\n",
      "pytest 8.3.3: Fri Sep 13 23:33:59 2024\n",
      "python-dateutil 2.9.0.post0: Fri Sep 13 23:33:59 2024\n",
      "python-dotenv 1.0.1: Fri Sep 13 23:33:23 2024\n",
      "pytz 2024.2: Fri Sep 13 23:33:04 2024\n",
      "pywin32 306: Fri Sep 13 23:37:33 2024\n",
      "PyYAML 6.0.2: Fri Sep 13 23:33:23 2024\n",
      "pyzmq 26.2.0: Fri Sep 13 23:37:38 2024\n",
      "regex 2024.9.11: Fri Sep 13 23:33:23 2024\n",
      "requests 2.32.3: Fri Sep 13 23:33:58 2024\n",
      "requests-oauthlib 2.0.0: Fri Sep 13 23:34:34 2024\n",
      "rich 13.8.1: Fri Sep 13 23:34:34 2024\n",
      "rsa 4.9: Fri Sep 13 23:33:58 2024\n",
      "s3transfer 0.10.2: Fri Sep 13 23:34:58 2024\n",
      "safetensors 0.4.5: Fri Sep 13 23:33:23 2024\n",
      "scikit-learn 1.5.2: Fri Sep 13 23:34:29 2024\n",
      "scipy 1.14.1: Fri Sep 13 23:33:50 2024\n",
      "sentence-transformers 2.7.0: Fri Sep 13 23:35:45 2024\n",
      "setuptools 65.5.0: Fri Sep 13 23:32:20 2024\n",
      "shellingham 1.5.4: Fri Sep 13 23:33:23 2024\n",
      "six 1.16.0: Fri Sep 13 23:33:23 2024\n",
      "sniffio 1.3.1: Fri Sep 13 23:33:22 2024\n",
      "soupsieve 2.6: Fri Sep 13 23:33:22 2024\n",
      "SQLAlchemy 2.0.34: Fri Sep 13 23:33:46 2024\n",
      "stack-data 0.6.3: Fri Sep 13 23:37:44 2024\n",
      "starlette 0.38.5: Fri Sep 13 23:34:28 2024\n",
      "striprtf 0.0.26: Fri Sep 13 23:33:04 2024\n",
      "sympy 1.13.2: Fri Sep 13 23:33:09 2024\n",
      "tbb 2021.13.1: Fri Sep 13 23:33:04 2024\n",
      "tenacity 8.5.0: Fri Sep 13 23:33:08 2024\n",
      "threadpoolctl 3.5.0: Fri Sep 13 23:33:08 2024\n",
      "tiktoken 0.7.0: Fri Sep 13 23:34:28 2024\n",
      "tokenizers 0.19.1: Fri Sep 13 23:34:58 2024\n",
      "tomli 2.0.1: Fri Sep 13 23:33:08 2024\n",
      "torch 2.3.0: Fri Sep 13 23:34:06 2024\n",
      "tornado 6.4.1: Fri Sep 13 23:37:38 2024\n",
      "tqdm 4.66.5: Fri Sep 13 23:33:45 2024\n",
      "traitlets 5.14.3: Fri Sep 13 23:37:37 2024\n",
      "transformers 4.44.2: Fri Sep 13 23:35:14 2024\n",
      "typer 0.12.5: Fri Sep 13 23:34:58 2024\n",
      "typing_extensions 4.12.2: Fri Sep 13 23:33:08 2024\n",
      "typing-inspect 0.9.0: Fri Sep 13 23:33:45 2024\n",
      "tzdata 2024.1: Fri Sep 13 23:33:08 2024\n",
      "urllib3 2.2.3: Fri Sep 13 23:33:07 2024\n",
      "uvicorn 0.30.6: Fri Sep 13 23:34:05 2024\n",
      "watchfiles 0.24.0: Fri Sep 13 23:34:05 2024\n",
      "wcwidth 0.2.13: Fri Sep 13 23:37:33 2024\n",
      "websockets 13.0.1: Fri Sep 13 23:33:07 2024\n",
      "websocket-client 1.8.0: Fri Sep 13 23:33:07 2024\n",
      "wrapt 1.16.0: Fri Sep 13 23:33:06 2024\n",
      "yarl 1.11.1: Fri Sep 13 23:34:05 2024\n",
      "zipp 3.20.2: Fri Sep 13 23:33:06 2024\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import distributions  \n",
    "import os, time\n",
    "\n",
    "for dist in distributions():\n",
    "    print(\"%s %s: %s\" % (dist.metadata[\"Name\"], dist.version, time.ctime(os.path.getctime(dist._path))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dist in distributions():\n",
    "    print(\"%s %s: %s\" % (dist.metadata[\"Name\"], dist.version, time.ctime(os.path.getctime(dist._path))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"chroma_llama1\"\n",
    "DATA_PATH = \"data\"\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "chroma_collection = chroma_client.create_collection(\"document_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miriy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "llm = Groq(model=\"llama3-8b-8192\")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(DATA_PATH).load_data()\n",
    "db = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "chroma_collection = db.get_or_create_collection(\"document_collection\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='077b0954-afb0-4465-a170-e9b5342c6a7a', embedding=None, metadata={'page_label': '31', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ab6d7e09-8359-41c6-9ff4-bb6cf03727c0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '31', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='0d0103f1b096966b6fb6bf1bab738db18daab0ab36f53f1b65aed8d2e2d9a4ca')}, text='Kick Starting an NLP Project | 31\\n8. Detect the sentence boundaries in the given text corpus and print the total \\nnumber of sentences. \\nNote\\nThe solution for this activity can be found via this link .\\nWe have learned about and achieved the preprocessing of given data. By now, you \\nshould be familiar with what NLP is and what basic preprocessing steps are needed \\nto carry out any NLP project. In the next section, we will focus on the different phases \\nof an NLP project.\\nKick Starting an NLP Project\\nWe can divide an NLP project into several sub-projects or phases. These phases are \\ncompleted in a particular sequence. This tends to increase the overall efficiency of the \\nprocess, as memory usage changes from one phase to the next. An NLP project has \\nto go through six major phases, which are outlined in the following figure:\\nFigure 1.4: Phases of an NLP project', mimetype='text/plain', start_char_idx=0, end_char_idx=870, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.44416866380839),\n",
       " NodeWithScore(node=TextNode(id_='74352274-d428-4d23-a6ac-89a7d0492999', embedding=None, metadata={'page_label': '8', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0b6c7c4f-3ab4-4b20-957d-64af8152c976', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '8', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='70ed61a6dc01d42e5f9d26a84f264e2d6dbcecefc6966642f3ba77f298b21599')}, text='8 | Introduction to Natural Language Processing\\nWe are now well acquainted with basic text analytics techniques.\\nNote\\nTo access the source code for this specific section, please refer  \\nto https://packt.live/38Yrf77 .\\nYou can also run this example online at https://packt.live/2ZsCvpf .\\nIn the next section, let\\'s dive deeper into the various steps and subtasks in NLP.\\nVarious Steps in NLP\\nWe\\'ve talked about the types of computations that are done with natural language. \\nApart from these basic tasks, you can also design your own tasks as per your \\nrequirements. In the coming sections, we will discuss the various preprocessing tasks \\nin detail and demonstrate each of them with an exercise. \\nTo perform these tasks, we will be using a Python library called NLTK  (Natural \\nLanguage Toolkit ). NLTK is a powerful open source tool that provides a set of \\nmethods and algorithms to perform a wide range of NLP tasks, including tokenizing, \\nparts-of-speech tagging, stemming, lemmatization, and more.\\nTokenization\\nTokenization  refers to the procedure of splitting a sentence into its constituent \\nparts—the words and punctuation that it is made up of. It is different from simply \\nsplitting the sentence on whitespaces, and instead actually divides the sentence \\ninto constituent words, numbers (if any), and punctuation, which may not always \\nbe separated by whitespaces. For example, consider this sentence: \"I am reading a \\nbook.\" Here, our task is to extract words/tokens from this sentence. After passing this \\nsentence to a tokenization program, the extracted words/tokens would be \"I,\" \"am,\" \\n\"reading,\" \"a,\" \"book,\" and \".\" – this example extracts one token at a time. Such tokens \\nare called unigrams . \\nNLTK provides a method called word_tokenize() , which tokenizes given text into \\nwords. It actually separates the text into different words based on punctuation and \\nspaces between words.\\nTo get a better understanding of tokenization, let\\'s solve an exercise based on it in \\nthe next section.', mimetype='text/plain', start_char_idx=0, end_char_idx=2007, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4405057726452505)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = index.as_retriever()\n",
    "\n",
    "relevant_docs = retriever.retrieve(\"WHAT are the various steps in NLP?\")\n",
    "\n",
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='bdb17fec-1ba1-4e54-83a8-5a28767e9902', embedding=None, metadata={'page_label': '37', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0a5b464f-e9b3-4a94-9c2d-a2b5e35ab6f8', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '37', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='d380ee1b71d2281397bd9849e302fbafb2fa8567eb6e1bd5902d343aef427827')}, text='Types of Data | 37\\nCategorizing Data Based on Structure\\nData can be divided on the basis of structure into three categories, namely, \\nstructured, semi-structured, and unstructured data, as shown in the  \\nfollowing diagram:\\nFigure 2.1: Categorization based on content\\nThese three categories are as follows:\\n• Structured data : This is the most organized form of data. It is represented in \\ntabular formats such as Excel files and Comma-Separated Value  (CSV) files. The \\nfollowing image shows what structured data usually looks like:\\nFigure 2.2: Structured data\\nThe preceding table contains information about five people, with each row \\nrepresenting a person and each column representing one of their attributes.', mimetype='text/plain', start_char_idx=0, end_char_idx=711, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.28312215778663063),\n",
       " NodeWithScore(node=TextNode(id_='af92849e-6882-43b3-ad1e-083a062c1d34', embedding=None, metadata={'page_label': '141', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='62dbe449-081e-4024-8850-0af08519011c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '141', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='3ac34792d5076a15ab97ca952735906dd5e21c17c2558a614a0732e060cf9930')}, text='Supervised Learning | 141\\nIn the next section, you will learn about regression, which is another type of \\nsupervised learning.\\nRegression\\nTo better understand regression, consider a practical example. For example, say  \\nyou have photos of several people, along with a list of their respective ages, and you \\nneed to predict the ages of some other people from their photos. This is a use case \\nfor regression. \\nIn the case of regression, the dependent variable (age, in this example) is continuous. \\nThe independent variables—that is, features—consist of the attributes of the images, \\nsuch as the color intensity of each pixel. Formally, regression analysis refers to the \\nprocess of learning a mapping function, which relates features or predictors (inputs) \\nto the dependent variable (output).\\nThere are various types of regression: univariate , multivariate , simple , multiple , \\nlinear , non-linear , polynomial  regression , stepwise  regression , ridge  regression , \\nlasso  regression , and elastic  net regression . If there is just one dependent variable, \\nthen it is referred to as univariate regression. On the other hand, two or more \\ndependent variables constitute multivariate regression. Simple regression has only \\none predictor or target variable, while multivariate regression has more than one \\npredictor variable. \\nSince linear regression in the base algorithm for all the different types of regression \\nmentioned previously, in the next section, we will cover linear regression in detail.\\nLinear Regression\\nThe term \"linear\" refers to the linearity of parameters. Parameters are the coefficients \\nof predictor variables in the linear regression equation. The following formula \\nrepresents the linear regression equation:\\nFigure 3.25: Formula for linear regression\\nHere, y is termed a dependent variable (output); it is continuous. X is an independent \\nvariable or feature (input). β0 and β1 are parameters. Є is the error component, \\nwhich is the difference between the actual and predicted values of y. Since linear \\nregression requires the variable to be linear, it is not used much in the real world. \\nHowever, it is useful for high-level predictions, such as the sales revenue of a product \\ngiven the price and advertising cost.', mimetype='text/plain', start_char_idx=0, end_char_idx=2254, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.2641071480264443)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x27abbcbc340>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k = 10) # similarity_top_k, streaming=True\n",
    "\n",
    "query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The various steps in NLP include tokenization, lemmatization, stemming, stop word removal, named entity recognition (NER), parts-of-speech (PoS) tagging, and chunking.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"WHAT are the various steps in NLP?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='077b0954-afb0-4465-a170-e9b5342c6a7a', embedding=None, metadata={'page_label': '31', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ab6d7e09-8359-41c6-9ff4-bb6cf03727c0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '31', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='0d0103f1b096966b6fb6bf1bab738db18daab0ab36f53f1b65aed8d2e2d9a4ca')}, text='Kick Starting an NLP Project | 31\\n8. Detect the sentence boundaries in the given text corpus and print the total \\nnumber of sentences. \\nNote\\nThe solution for this activity can be found via this link .\\nWe have learned about and achieved the preprocessing of given data. By now, you \\nshould be familiar with what NLP is and what basic preprocessing steps are needed \\nto carry out any NLP project. In the next section, we will focus on the different phases \\nof an NLP project.\\nKick Starting an NLP Project\\nWe can divide an NLP project into several sub-projects or phases. These phases are \\ncompleted in a particular sequence. This tends to increase the overall efficiency of the \\nprocess, as memory usage changes from one phase to the next. An NLP project has \\nto go through six major phases, which are outlined in the following figure:\\nFigure 1.4: Phases of an NLP project', mimetype='text/plain', start_char_idx=0, end_char_idx=870, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.44416866380839),\n",
       " NodeWithScore(node=TextNode(id_='74352274-d428-4d23-a6ac-89a7d0492999', embedding=None, metadata={'page_label': '8', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0b6c7c4f-3ab4-4b20-957d-64af8152c976', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '8', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='70ed61a6dc01d42e5f9d26a84f264e2d6dbcecefc6966642f3ba77f298b21599')}, text='8 | Introduction to Natural Language Processing\\nWe are now well acquainted with basic text analytics techniques.\\nNote\\nTo access the source code for this specific section, please refer  \\nto https://packt.live/38Yrf77 .\\nYou can also run this example online at https://packt.live/2ZsCvpf .\\nIn the next section, let\\'s dive deeper into the various steps and subtasks in NLP.\\nVarious Steps in NLP\\nWe\\'ve talked about the types of computations that are done with natural language. \\nApart from these basic tasks, you can also design your own tasks as per your \\nrequirements. In the coming sections, we will discuss the various preprocessing tasks \\nin detail and demonstrate each of them with an exercise. \\nTo perform these tasks, we will be using a Python library called NLTK  (Natural \\nLanguage Toolkit ). NLTK is a powerful open source tool that provides a set of \\nmethods and algorithms to perform a wide range of NLP tasks, including tokenizing, \\nparts-of-speech tagging, stemming, lemmatization, and more.\\nTokenization\\nTokenization  refers to the procedure of splitting a sentence into its constituent \\nparts—the words and punctuation that it is made up of. It is different from simply \\nsplitting the sentence on whitespaces, and instead actually divides the sentence \\ninto constituent words, numbers (if any), and punctuation, which may not always \\nbe separated by whitespaces. For example, consider this sentence: \"I am reading a \\nbook.\" Here, our task is to extract words/tokens from this sentence. After passing this \\nsentence to a tokenization program, the extracted words/tokens would be \"I,\" \"am,\" \\n\"reading,\" \"a,\" \"book,\" and \".\" – this example extracts one token at a time. Such tokens \\nare called unigrams . \\nNLTK provides a method called word_tokenize() , which tokenizes given text into \\nwords. It actually separates the text into different words based on punctuation and \\nspaces between words.\\nTo get a better understanding of tokenization, let\\'s solve an exercise based on it in \\nthe next section.', mimetype='text/plain', start_char_idx=0, end_char_idx=2007, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4405057726452505),\n",
       " NodeWithScore(node=TextNode(id_='d998aca4-eae9-46ee-b664-546a377d1d3a', embedding=None, metadata={'page_label': '1', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4ecf2479-b988-484a-8234-823351047a63', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='0c3b54d4707643a70ee5d43f204b7cfb75ffa99f7d1491cc3242ccac90844778')}, text='Overview\\nIn this chapter, you will learn the difference between Natural Language \\nProcessing  (NLP) and basic text analytics. You will implement various \\npreprocessing tasks such as tokenization, lemmatization, stemming, stop \\nword removal, and more. By the end of this chapter, you will have a deep \\nunderstanding of the various phases of an NLP project, from data collection \\nto model deployment.Introduction to Natural \\nLanguage Processing1', mimetype='text/plain', start_char_idx=0, end_char_idx=443, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.43202259595032383),\n",
       " NodeWithScore(node=TextNode(id_='b9e88955-1868-4763-8a71-6bc62c1ac82e', embedding=None, metadata={'page_label': '24', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='48be278a-d850-40ca-929e-37997b44790c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '24', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='54d0cb3debbf8ea4b60ebef6653c88642ac21e62f07a9913015f5bf090bb6f6b')}, text=\"24 | Introduction to Natural Language Processing\\nHence, we have learned how to use the lemmatization process to transform a \\ngiven word into its base form.\\nNote\\nTo access the source code for this specific section, please refer  \\nto https://packt.live/3903ETS .\\nYou can also run this example online at https://packt.live/2Wlqu33 .\\nIn the next section, we will look at another preprocessing step in NLP: named entity \\nrecognition  (NER ).\\nNamed Entity Recognition (NER)\\nNER is the process of extracting important entities, such as person names, place \\nnames, and organization names, from some given text. These are usually not present \\nin dictionaries. So, we need to treat them differently. The main objective of this \\nprocess is to identify the named entities (such as proper nouns) and map them to \\ncategories, which are already defined. For example, categories might include names \\nof people, places, and so on.\\nNER has found use in many NLP tasks, including assigning tags to news articles, \\nsearch algorithms, and more. NER can analyze a news article and extract the major \\npeople, organizations, and places discussed in it and assign them as tags for  \\nnew articles. \\nIn the case of search algorithms, let's suppose we have to create a search engine, \\nmeant specifically for books. If we were to submit a given query for all the words, \\nthe search would take a lot of time. Instead, if we extract the top entities from all the \\nbooks using NER and run a search query on the entities rather than all the content, \\nthe speed of the system would increase dramatically.\\nTo get a better understanding of this process, we'll perform an exercise. Before \\nmoving on to the exercise, let me introduce you to chunking, which we are going to \\nuse in the following exercise. Chunking is the process of grouping words together into \\nchunks, which can be further used to find noun groups and verb groups, or can also \\nbe used for sentence partitioning.\", mimetype='text/plain', start_char_idx=0, end_char_idx=1943, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4195684464073517),\n",
       " NodeWithScore(node=TextNode(id_='52c46f75-415e-4eba-81e7-b965be25e60a', embedding=None, metadata={'page_label': '1', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='849d347e-8016-4cfd-ba9a-362297b7acbe', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='e3e663b532cba6dab084a92efab13bbc4edf4bd1721f44b04598774f23103113')}, text='Rohan Chopra, Aniruddha M. Godbole, Nipun Sadvilkar,  \\nMuzaffar Bashir Shah, Sohom Ghosh, and Dwight GunningConfidently design and build your own NLP projects \\nwith this easy-to-understand practical guide The \\nNatural  \\nLanguage \\nProcessing  \\nWorkshop', mimetype='text/plain', start_char_idx=0, end_char_idx=251, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.41841221155046326),\n",
       " NodeWithScore(node=TextNode(id_='ae25b3be-ea58-4d99-9902-6eda566e133c', embedding=None, metadata={'page_label': '10', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6e5286a0-841b-4492-b3cf-3b2897706a25', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '10', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='f77f23a2be7fcdd2dbbf21f505810f78b7c032578a00da763dc410eeaf60da03')}, text='10 | Introduction to Natural Language Processing\\nWe can see the list of tokens generated with the help of the  \\nword_tokenize()  method.\\nNote\\nTo access the source code for this specific section, please refer  \\nto https://packt.live/30bGG85 .\\nYou can also run this example online at https://packt.live/30dK1mZ .\\nIn the next section, we will see another pre-processing step:  \\nParts-of-Speech (PoS) tagging .\\nPoS Tagging\\nIn NLP, the term PoS refers to parts of speech. PoS tagging refers to the process \\nof tagging words within sentences with their respective PoS. We extract the PoS of \\ntokens constituting a sentence so that we can filter out the PoS that are of interest \\nand analyze them. For example, if we look at the sentence, \"The sky is blue,\" we get \\nfour tokens, namely \"The,\" \"sky,\" \"is,\" and \"blue\", with the help of tokenization. Now, \\nusing a PoS tagger , we tag the PoS for each word/token. This will look as follows:\\n[(\\'The\\', \\'DT\\'), (\\'sky\\', \\'NN\\'), (\\'is\\', \\'VBZ\\'), (\\'blue\\', \\'JJ\\')]\\nThe preceding format is an output of the NLTK pos_tag() method. It is a list of \\ntuples in which every tuple consists of the word followed by the PoS tag:\\nDT = Determiner\\nNN = Noun, common, singular or mass\\nVBZ  = Verb, present tense, third-person singular\\nJJ = Adjective\\nFor the complete list of PoS tags in NLTK, you can refer  \\nto https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/ .\\nPoS tagging is performed using different techniques, one of which is a rule-based \\napproach that builds a list to assign a possible tag for each word.', mimetype='text/plain', start_char_idx=0, end_char_idx=1561, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4138943336967952),\n",
       " NodeWithScore(node=TextNode(id_='2e91847c-8012-40d9-9a57-1c7e78304efd', embedding=None, metadata={'page_label': '11', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f31e23bf-72c9-4fcb-8945-2ba9798fc5cb', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='751de2008d64b89a507a24c7d867c5a117fb6e83e0d465670f9f4747ad240676')}, text='Various Steps in NLP | 11\\nPoS tagging finds application in many NLP tasks, including word sense \\ndisambiguation, classification, Named Entity Recognition  (NER ), and coreference \\nresolution. For example, consider the usage of the word \"planted\" in these two \\nsentences: \"He planted the evidence for the case \" and \" He planted five trees in \\nthe garden. \" We can see that the PoS tag of \"planted\" would clearly help us in \\ndifferentiating between the different meanings of the sentences.\\nLet\\'s perform a simple exercise to understand how PoS tagging is done in Python.\\nExercise 1.03: PoS Tagging\\nIn this exercise, we will find out the PoS for each word in the sentence, I am \\nreading NLP Fundamentals . We first make use of tokenization in order to get \\nthe tokens. Later, we will use the pos_tag()  method, which will help us find the PoS \\nfor each word/token. Follow these steps to implement this exercise:\\n1. Open a Jupyter Notebook.\\n2. Insert a new cell and add the following code to import the necessary libraries:\\nfrom nltk import word_tokenize, pos_tag\\n3. To find the tokens in the sentence, we make use of the word_tokenize()  \\nmethod. Insert a new cell and add the following code to implement this:\\ndef get_tokens(sentence):\\n    words = word_tokenize(sentence)\\n    return words\\n4. Print the tokens with the help of the print()  function. To implement this,  \\nadd a new cell and write the following code:\\nwords  = get_tokens(\"I am reading NLP Fundamentals\")\\nprint(words)\\nThis code generates the following output:\\n[\\'I\\', \\'am\\', \\'reading\\', \\'NLP\\', \\'Fundamentals\\']', mimetype='text/plain', start_char_idx=0, end_char_idx=1567, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4049058225951022),\n",
       " NodeWithScore(node=TextNode(id_='f17e582b-c192-48ef-9291-a6d4306fa8fa', embedding=None, metadata={'page_label': '9', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1e1f272b-7f6f-48a3-87e3-6307683d377b', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='3c2b6cd374fa061ab703734eb825d2f29b38ce30224f0c503fe6d633c5603f56')}, text='Various Steps in NLP | 9\\nExercise 1.02: Tokenization of a Simple Sentence\\nIn this exercise, we will tokenize the words in a given sentence with the help of the \\nNLTK  library. Follow these steps to implement this exercise using the sentence, \"I am \\nreading NLP Fundamentals.\"\\n1. Open a Jupyter Notebook.\\n2. Insert a new cell and add the following code to import the necessary libraries \\nand download the different types of NLTK data that we are going to use for \\ndifferent tasks in the following exercises:\\nfrom nltk import word_tokenize, download\\ndownload([\\'punkt\\',\\'averaged_perceptron_tagger\\',\\'stopwords\\'])\\nIn the preceding code, we are using NLTK\\'s download()  method, which \\ndownloads the given data from NLTK. NLTK data contains different corpora \\nand trained models. In the preceding example, we will be downloading the stop \\nword list, \\'punkt\\' , and a perceptron tagger, which is used to implement parts \\nof speech tagging using a structured algorithm. The data will be downloaded at \\nnltk_data/corpora/  in the home directory of your computer. Then, it will \\nbe loaded from the same path in further steps.\\n3. The word_tokenize()  method is used to split the sentence into words/\\ntokens. We need to add a sentence as input to the word_tokenize()  method \\nso that it performs its job. The result obtained will be a list, which we will store in \\na word  variable. To implement this, insert a new cell and add the following code:\\ndef get_tokens(sentence):\\n    words = word_tokenize(sentence)\\n    return words\\n4. In order to view the list of tokens generated, we need to view it using the \\nprint()  function. Insert a new cell and add the following code to  \\nimplement this:\\nprint(get_tokens(\"I am reading NLP Fundamentals.\"))\\nThis code generates the following output:\\n[\\'I\\', \\'am\\', \\'reading\\', \\'NLP\\', \\'Fundamentals\\', \\'.\\']', mimetype='text/plain', start_char_idx=0, end_char_idx=1822, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4041989442844693),\n",
       " NodeWithScore(node=TextNode(id_='f7d8bad0-e1ac-46cb-ab70-37d6a579cf53', embedding=None, metadata={'page_label': '25', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='bb554dc8-8d91-4fd7-9652-a5a9bfd44dcf', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '25', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='e74f93b8f82ff26aedd87df5b171b36ce8e7198d900d72d58604a344cf5f0aaa')}, text='Various Steps in NLP | 25\\nExercise 1.09: Treating Named Entities\\nIn this exercise, we will find the named entities in a given sentence. Follow these steps \\nto implement this exercise using the following sentence:\\n\"We are reading a book published by Packt which is based out of Birmingham.\"\\n1. Open a Jupyter Notebook.\\n2. Insert a new cell and add the following code to import the necessary libraries:\\nfrom nltk import download\\nfrom nltk import pos_tag\\nfrom nltk import ne_chunk\\nfrom nltk import word_tokenize\\ndownload(\\'maxent_ne_chunker\\')\\ndownload(\\'words\\')\\n3. Declare the sentence  variable and assign it a string. Insert a new cell and add \\nthe following code to implement this:\\nsentence = \"We are reading a book published by Packt \"\\\\\\n           \"which is based out of Birmingham.\"\\n4. To find the named entities from the preceding text, insert a new cell and add the \\nfollowing code:\\ndef get_ner(text):\\n    i = ne_chunk(pos_tag(word_tokenize(text)), binary=True)\\n    return [a for a in i if len(a)==1]\\nget_ner(sentence)\\nThis code generates the following output:\\n[Tree(\\'NE\\', [(\\'Packt\\', \\'NNP\\')]), Tree(\\'NE\\', [(\\'Birmingham\\', \\'NNP\\')])]', mimetype='text/plain', start_char_idx=0, end_char_idx=1132, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4029171833255556),\n",
       " NodeWithScore(node=TextNode(id_='f2e4d724-5204-413e-8fbe-11688414ed0c', embedding=None, metadata={'page_label': '23', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3f02e916-4457-4597-baf5-99b0a7c415cb', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '23', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='8f7a80cad22543e933f0449a26a572742ccf549e896c976da4e90b468c8cf4a0')}, text=\"Various Steps in NLP | 23\\nExercise 1.08: Extracting the Base Word Using Lemmatization\\nIn this exercise, we will use the lemmatization process to produce the proper form of \\na given word. Follow these steps to implement this exercise:\\n1. Open a Jupyter Notebook.\\n2. Insert a new cell and add the following code to import the necessary libraries:\\nfrom nltk import download\\ndownload('wordnet')\\nfrom nltk.stem.wordnet import WordNetLemmatizer\\n3. Create an object of the WordNetLemmatizer  class. Insert a new cell and add \\nthe following code to implement this:\\nlemmatizer = WordNetLemmatizer()\\n4. Bring the word to its proper form by using the lemmatize()  method of the \\nWordNetLemmatizer  class. Insert a new cell and add the following code to \\nimplement this:\\ndef get_lemma(word):\\n    return lemmatizer.lemmatize(word)\\nget_lemma('products')\\nWith the input products , the following output is generated:\\n'product'\\n5. Similarly, use the input as production  now:\\nget_lemma('production')\\nWith the input production , the following output is generated:\\n'production'\\n6. Similarly, use the input as coming  now:\\nget_lemma('coming')\\nWith the input coming , the following output is generated:\\n'coming'\", mimetype='text/plain', start_char_idx=0, end_char_idx=1190, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3944083272848286)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The development of NLP as a field of computer science is a relatively recent phenomenon. It has its roots in the 1950s, when the first attempts were made to develop machines that could understand and generate human language.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"WHAT IS history of NLP?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='dd284adc-119a-41bd-9cd9-a2977b5983f1', embedding=None, metadata={'page_label': '2', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9b1b5cbc-d982-4c60-9e30-23c0550dde1a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '2', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='5f3ec1a84bd488a13668023a6310d4d0fb66972f376e815cfe23d039bfcdb440')}, text=\"2 | Introduction to Natural Language Processing\\nIntroduction\\nBefore we can get into NLP in any depth, we first need to understand what natural \\nlanguage is. To put it in simple terms, it is a means for us to express our thoughts \\nand ideas. To define it more specifically, language is a mutually agreed upon set of \\nprotocols involving words/sounds that we use to communicate with each other.\\nIn this era of digitization and computation, we are constantly interacting with \\nmachines around us through various means, such as voice commands and \\ntyping instructions in the form of words. Thus, it has become essential to develop \\nmechanisms by which human language can be comprehended accurately by \\ncomputers. NLP helps us do this. So, NLP can be defined as a field of computer \\nscience that is concerned with enabling computer algorithms to understand, analyze, \\nand generate natural languages.\\nLet's look at an example. You have probably interacted with Siri or Alexa at some \\npoint. Ask Alexa for a cricket score, and it will reply with the current score. The \\ntechnology behind this is NLP. Siri and Alexa use techniques such as Speech to Text \\nwith the help of a search engine to do this magic. As the name suggests, Speech to \\nText is an application of NLP in which computers are trained to understand verbally \\nspoken words.\\nNLP works at different levels, which means that machines process and understand \\nnatural language at different levels. These levels are as follows:\\n• Morphological level : This level deals with understanding word structure and \\nword information.\\n• Lexical level : This level deals with understanding the part of speech of the word.\\n• Syntactic level : This level deals with understanding the syntactic analysis of a \\nsentence, or parsing a sentence.\\n• Semantic level : This level deals with understanding the actual meaning  \\nof a sentence.\\n• Discourse level : This level deals with understanding the meaning of a sentence \\nbeyond just the sentence level, that is, considering the context.\\n• Pragmatic level : This level deals with using real-world knowledge to understand \\nthe sentence.\", mimetype='text/plain', start_char_idx=0, end_char_idx=2118, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.39406069481835826),\n",
       " NodeWithScore(node=TextNode(id_='d0229df6-e89b-4630-bac5-c88b8ea22de2', embedding=None, metadata={'page_label': '1', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c29ff68f-2786-4327-981c-550928d3837d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='0c3b54d4707643a70ee5d43f204b7cfb75ffa99f7d1491cc3242ccac90844778')}, text='Overview\\nIn this chapter, you will learn the difference between Natural Language \\nProcessing  (NLP) and basic text analytics. You will implement various \\npreprocessing tasks such as tokenization, lemmatization, stemming, stop \\nword removal, and more. By the end of this chapter, you will have a deep \\nunderstanding of the various phases of an NLP project, from data collection \\nto model deployment.Introduction to Natural \\nLanguage Processing1', mimetype='text/plain', start_char_idx=0, end_char_idx=443, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.39128112741683885),\n",
       " NodeWithScore(node=TextNode(id_='ac95f26e-3c31-4164-834b-87f68aee0fab', embedding=None, metadata={'page_label': '10', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='63a5c5a3-2cbc-497d-b5a7-97cb87ab9a85', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '10', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='f77f23a2be7fcdd2dbbf21f505810f78b7c032578a00da763dc410eeaf60da03')}, text='10 | Introduction to Natural Language Processing\\nWe can see the list of tokens generated with the help of the  \\nword_tokenize()  method.\\nNote\\nTo access the source code for this specific section, please refer  \\nto https://packt.live/30bGG85 .\\nYou can also run this example online at https://packt.live/30dK1mZ .\\nIn the next section, we will see another pre-processing step:  \\nParts-of-Speech (PoS) tagging .\\nPoS Tagging\\nIn NLP, the term PoS refers to parts of speech. PoS tagging refers to the process \\nof tagging words within sentences with their respective PoS. We extract the PoS of \\ntokens constituting a sentence so that we can filter out the PoS that are of interest \\nand analyze them. For example, if we look at the sentence, \"The sky is blue,\" we get \\nfour tokens, namely \"The,\" \"sky,\" \"is,\" and \"blue\", with the help of tokenization. Now, \\nusing a PoS tagger , we tag the PoS for each word/token. This will look as follows:\\n[(\\'The\\', \\'DT\\'), (\\'sky\\', \\'NN\\'), (\\'is\\', \\'VBZ\\'), (\\'blue\\', \\'JJ\\')]\\nThe preceding format is an output of the NLTK pos_tag() method. It is a list of \\ntuples in which every tuple consists of the word followed by the PoS tag:\\nDT = Determiner\\nNN = Noun, common, singular or mass\\nVBZ  = Verb, present tense, third-person singular\\nJJ = Adjective\\nFor the complete list of PoS tags in NLTK, you can refer  \\nto https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/ .\\nPoS tagging is performed using different techniques, one of which is a rule-based \\napproach that builds a list to assign a possible tag for each word.', mimetype='text/plain', start_char_idx=0, end_char_idx=1561, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3880320138713051),\n",
       " NodeWithScore(node=TextNode(id_='c02455f2-fccf-41ce-8b91-4ce50d94a53e', embedding=None, metadata={'page_label': '3', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7276d6b7-15e1-4796-81d5-41ab57deabb7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='b5af78d0befb53e16e5be370adc5aa9daa5a9ae95ede8755c04dd14b3fb04e05')}, text=\"History of NLP | 3\\nHistory of NLP\\nNLP is a field that has emerged from various other fields such as artificial intelligence, \\nlinguistics, and data science. With the advancement of computing technologies and \\nthe increased availability of data, NLP has undergone a huge change. Previously, \\na traditional rule-based system was used for computations, in which you had to \\nexplicitly write hardcoded rules. Today, computations on natural language are being \\ndone using machine learning and deep learning techniques.\\nConsider an example. Let's say we have to extract the names of some politicians from \\na set of political news articles. So, if we want to apply rule-based grammar, we must \\nmanually craft certain rules based on human understanding of language. Some of the \\nrules for extracting a person's name can be that the word should be a proper noun, \\nevery word should start with a capital letter, and so on. As we can see, using a rule-\\nbased system like this would not yield very accurate results.\\nRule-based systems do work well in some cases, but the disadvantages far outweigh \\nthe advantages. One major disadvantage is that the same rule cannot be applicable \\nin all cases, given the complex and nuanced nature of most language. These \\ndisadvantages can be overcome by using machine learning, where we write an \\nalgorithm that tries to learn a language using the text corpus (training data) rather \\nthan us explicitly programming it to do so.\\nText Analytics and NLP\\nText analytics  is the method of extracting meaningful insights and answering \\nquestions from text data, such as those to do with the length of sentences, length of \\nwords, word count, and finding words from the text. Let's understand this with  \\nan example.\\nSuppose we are doing a survey using news articles. Let's say we have to find the top \\nfive countries that contributed the most in the field of space technology in the past 5 \\nyears. So, we will collect all the space technology-related news from the past 5 years \\nusing the Google News API. Now, we must extract the names of countries in these \\nnews articles. We can perform this task using a file containing a list of all the countries \\nin the world.\", mimetype='text/plain', start_char_idx=0, end_char_idx=2185, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3852271828852972),\n",
       " NodeWithScore(node=TextNode(id_='c19ddc91-96cf-4037-9e46-9b1c068097c6', embedding=None, metadata={'page_label': '1', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='42d2136c-1548-4008-9f24-99f05e9a946d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='e3e663b532cba6dab084a92efab13bbc4edf4bd1721f44b04598774f23103113')}, text='Rohan Chopra, Aniruddha M. Godbole, Nipun Sadvilkar,  \\nMuzaffar Bashir Shah, Sohom Ghosh, and Dwight GunningConfidently design and build your own NLP projects \\nwith this easy-to-understand practical guide The \\nNatural  \\nLanguage \\nProcessing  \\nWorkshop', mimetype='text/plain', start_char_idx=0, end_char_idx=251, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3726612896948831),\n",
       " NodeWithScore(node=TextNode(id_='0d2d15ce-4fc3-4b52-b966-3bba70c3a282', embedding=None, metadata={'page_label': '30', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='333fbe96-c1c8-4d94-b29f-c7f4f8ed1b3e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '30', 'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf', 'file_type': 'application/pdf', 'file_size': 59957721, 'creation_date': '2024-09-02', 'last_modified_date': '2024-08-22'}, hash='9f73ce9a29dcc669f1b667ae055c67fec6d4dfcc933f481ef2b2501f3f8d4cba')}, text='30 | Introduction to Natural Language Processing\\nAs you can see in the code, the sent_tokenize  method is able to differentiate \\nbetween the period (.) after \"Mr\" and the one used to end the sentence. We have \\ncovered all the preprocessing steps that are involved in NLP. \\nNote\\nTo access the source code for this specific section, please refer  \\nto https://packt.live/2ZseU86 .\\nYou can also run this example online at https://packt.live/2CC8Ukp .\\nNow, using the knowledge we\\'ve gained, let\\'s perform an activity.\\nActivity 1.01: Preprocessing of Raw Text\\nWe have a text corpus that is in an improper format. In this activity, we will perform \\nall the preprocessing steps that were discussed earlier to get some meaning out of \\nthe text.\\nNote\\nThe text corpus, file.txt , can be found at this location:  \\nhttps://packt.live/30cu54z\\nAfter downloading the file, place it in the same directory as the notebook.\\nFollow these steps to implement this activity:\\n1. Import the necessary libraries.\\n2. Load the text corpus to a variable.\\n3. Apply the tokenization process to the text corpus and print the first 20 tokens.\\n4. Apply spelling correction on each token and print the initial 20 corrected tokens \\nas well as the corrected text corpus.\\n5. Apply PoS tags to each of the corrected tokens and print them.\\n6. Remove stop words from the corrected token list and print the initial 20 tokens.\\n7. Apply stemming and lemmatization to the corrected token list and then print the \\ninitial 20 tokens.', mimetype='text/plain', start_char_idx=0, end_char_idx=1486, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.35618245877757554)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'054d1ad3-b786-4d0d-8d3f-d2a31769bb5d': {'page_label': '37',\n",
       "  'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf',\n",
       "  'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 59957721,\n",
       "  'creation_date': '2024-09-02',\n",
       "  'last_modified_date': '2024-08-22'},\n",
       " '47043163-5ba8-4d0b-92d6-c1c725a4b57e': {'page_label': '141',\n",
       "  'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf',\n",
       "  'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 59957721,\n",
       "  'creation_date': '2024-09-02',\n",
       "  'last_modified_date': '2024-08-22'},\n",
       " '54d3b01c-bc8a-432e-9674-344eb098d1e2': {'page_label': '7',\n",
       "  'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf',\n",
       "  'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 59957721,\n",
       "  'creation_date': '2024-09-02',\n",
       "  'last_modified_date': '2024-08-22'},\n",
       " 'f6d8bd32-4ef1-4fc7-b6df-fa7e60277b8c': {'page_label': '124',\n",
       "  'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf',\n",
       "  'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 59957721,\n",
       "  'creation_date': '2024-09-02',\n",
       "  'last_modified_date': '2024-08-22'},\n",
       " '730407fa-aba7-4748-b202-15833cd89e53': {'page_label': '40',\n",
       "  'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf',\n",
       "  'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 59957721,\n",
       "  'creation_date': '2024-09-02',\n",
       "  'last_modified_date': '2024-08-22'},\n",
       " 'b81dc98e-fce6-4aa7-843b-ae06b8d47661': {'page_label': '198',\n",
       "  'file_name': 'THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf',\n",
       "  'file_path': 'd:\\\\Python\\\\university_chatbot\\\\data\\\\THE_NATURAL_LANGUAGE_PROCESSING_WORKSHOP.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 59957721,\n",
       "  'creation_date': '2024-09-02',\n",
       "  'last_modified_date': '2024-08-22'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
